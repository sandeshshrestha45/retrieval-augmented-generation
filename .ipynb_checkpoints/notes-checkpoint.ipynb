{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1d471c-745b-4925-ae17-4cac826c5660",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "It was introduced in the paper [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401).\n",
    "\n",
    "Each step can be roughly broken down to:\n",
    "\n",
    "* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n",
    "* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
    "* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt.\n",
    "\n",
    "## Why RAG?\n",
    "\n",
    "The main goal of RAG is to improve the generation outptus of LLMs.\n",
    "\n",
    "Two primary improvements can be seen as:\n",
    "1. **Preventing hallucinations** - LLMs are incredible but they are prone to potential hallucination, as in, generating something that *looks* correct but isn't. RAG pipelines can help LLMs generate more factual outputs by providing them with factual (retrieved) inputs. And even if the generated answer from a RAG pipeline doesn't seem correct, because of retrieval, you also have access to the sources where it came from.\n",
    "2. **Work with custom data** - Many base LLMs are trained with internet-scale text data. This means they have a great ability to model language, however, they often lack specific knowledge. RAG systems can provide LLMs with domain-specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.\n",
    "\n",
    "The authors of the original RAG paper mentioned above outlined these two points in their discussion.\n",
    "\n",
    "> This work offers several positive societal benefits over previous work: the fact that it is more\n",
    "strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\n",
    "with generations that are more factual, and offers more control and interpretability. RAG could be\n",
    "employed in a wide variety of scenarios with direct benefit to society, for example by endowing it\n",
    "with a medical index and asking it open-domain questions on that topic, or by helping people be more\n",
    "effective at their jobs.\n",
    "\n",
    "RAG can also be a much quicker solution to implement than fine-tuning an LLM on specific data. \n",
    "\n",
    "\n",
    "## What kind of problems can RAG be used for?\n",
    "\n",
    "RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).\n",
    "\n",
    "For example you could use RAG for:\n",
    "* **Customer support Q&A chat** - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\". Klarna, a large financial company, [uses a system like this](https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/) to save $40M per year on customer support costs.\n",
    "* **Email chain analysis** - Let's say you're an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.\n",
    "* **Company internal documentation chat** - If you've worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn't suffice.\n",
    "* **Textbook Q&A** - Let's say you're studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.\n",
    "\n",
    "All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.\n",
    "\n",
    "From this angle, you can consider an LLM a calculator for words.\n",
    "\n",
    "## Why local?\n",
    "\n",
    "Privacy, speed, cost.\n",
    "\n",
    "Running locally means you use your own hardware.\n",
    "\n",
    "From a privacy standpoint, this means you don't have send potentially sensitive data to an API.\n",
    "\n",
    "From a speed standpoint, it means you won't necessarily have to wait for an API queue or downtime, if your hardware is running, the pipeline can run.\n",
    "\n",
    "And from a cost standpoint, running on your own hardware often has a heavier starting cost but little to no costs after that.\n",
    "\n",
    "Performance wise, LLM APIs may still perform better than an open-source model running locally on general tasks but there are more and more examples appearing of smaller, focused models outperforming larger models. \n",
    "\n",
    "## Key terms\n",
    "\n",
    "| Term | Description |\n",
    "| ----- | ----- | \n",
    "| **Token** | A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word,<br> part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br> Text gets broken into tokens before being passed to an LLM. |\n",
    "| **Embedding** | A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with<br> 768 values. Similar pieces of text (in meaning) will ideally have similar values. |\n",
    "| **Embedding model** | A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 <br>tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model. |\n",
    "| **Similarity search/vector search** | Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, <br>two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about<br> different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity. |\n",
    "| **Large Language Model (LLM)** | A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. <br>For example, given a sequence of the text \"hello, world!\", a genertive LLM may produce \"we're going to build a RAG pipeline today!\".<br> This generation will be highly dependant on the training data and prompt. |\n",
    "| **LLM context window** | The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens<br> (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context<br> window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information<br> to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items<br> from the retrieval system to aid with its generation. |\n",
    "| **Prompt** | A common term for describing the input to a generative LLM. The idea of \"[prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)\" is to structure a text-based<br> (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is<br> possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown <br>the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like \"may output\" are used). | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc04e3-9bde-4913-8bf8-0c032190526c",
   "metadata": {},
   "source": [
    "### Chunking and embedding questions\n",
    "#### Which embedding model should I use?\n",
    "\n",
    "This depends on many factors. My best advice is to experiment, experiment, experiment!\n",
    "\n",
    "If you want the model to run locally, you'll have to make sure it's feasible to run on your own hardware.\n",
    "\n",
    "A good place to see how different models perform on a wide range of embedding tasks is the Hugging Face Massive Text Embedding Benchmark (MTEB) Leaderboard.\n",
    "\n",
    "#### What other forms of text chunking/splitting are there?\n",
    "\n",
    "There are a fair few options here too. We've kept it simple with groups of sentences.\n",
    "\n",
    "For more, Pinecone has a great guide on different kinds of chunking including for different kinds of data such as markdown and LaTeX.\n",
    "\n",
    "Libraries such as LangChain also have a good amount of in-built text splitting options.\n",
    "\n",
    "#### What should I think about when creating my embeddings?\n",
    "\n",
    "Our model turns text inputs up to 384 tokens long in embedding vectors of size 768.\n",
    "\n",
    "Generally, the larger the vector size, the more information that gets encoded into the embedding (however, this is not always the case, as smaller, better models can outperform larger ones).\n",
    "\n",
    "Though with larger vector sizes comes larger storage and compute requirements.\n",
    "\n",
    "Our model is also relatively small (420MB) in size compared to larger models that are available.\n",
    "\n",
    "Larger models may result in better performance but will also require more compute.\n",
    "\n",
    "So some things to think about:\n",
    "\n",
    "* Size of input - If you need to embed longer sequences, choose a model with a larger input capacity.\n",
    "* Size of embedding vector - Larger is generally a better representation but requires more compute/storage.\n",
    "* Size of model - Larger models generally result in better embeddings but require more compute power/time to run.\n",
    "* Open or closed - Open models allow you to run them on your own hardware whereas closed models can be easier to setup but * require an API call to get embeddings.\n",
    "* \n",
    "#### Where should I store my embeddings?\n",
    "\n",
    "If you've got a relatively small dataset, for example, under 100,000 examples (this number is rough and only based on first hand experience), np.array or torch.tensor can work just fine as your dataset.\n",
    "\n",
    "But if you've got a production system and want to work with 100,000+ embeddings, you may want to look into a vector database (these have become very popular lately and there are many offerings).\n",
    "\n",
    "Document Ingestion and Embedding Creation Extensions\n",
    "One major extension to the workflow above would to functionize it.\n",
    "\n",
    "Or turn it into a script.\n",
    "\n",
    "As in, take all the functionality we've created and package it into a single process (e.g. go from document -> embeddings file).\n",
    "\n",
    "So you could input a document on one end and have embeddings come out the other end. The hardest part of this is knowing what kind of preprocessing your text may need before it's turned into embeddings. Cleaner text generally means better results.\n",
    "\n",
    "### RAG - Search and Answer\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n",
    "\n",
    "Let's breakdown each step:\n",
    "\n",
    "* Retrieval - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n",
    "* Augmentation - LLMs are capable of generating text given a prompt. However, this generated text is designed to look right. And it often has some correct information, however, they are prone to hallucination (generating a result that looks like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n",
    "* Generation - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n",
    "* \n",
    "The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n",
    "\n",
    "This is an incredibly helpful tool.\n",
    "\n",
    "Let's say you had 1000s of customer support documents.\n",
    "\n",
    "You could use RAG to generate direct answers to questions with links to relevant documentation.\n",
    "\n",
    "Or you were an insurance company with large chains of claims emails.\n",
    "\n",
    "You could use RAG to answer questions about the emails with sources.\n",
    "\n",
    "One helpful analogy is to think of LLMs as calculators for words.\n",
    "\n",
    "With good inputs, the LLM can sort them into helpful outputs.\n",
    "\n",
    "How?\n",
    "\n",
    "It starts with better search.\n",
    "\n",
    "#### Similarity search\n",
    "Similarity search or semantic search or vector search is the idea of searching on vibe.\n",
    "\n",
    "If this sounds like woo, woo. It's not.\n",
    "\n",
    "Perhaps searching via meaning is a better analogy.\n",
    "\n",
    "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
    "\n",
    "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
    "\n",
    "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n",
    "\n",
    "Example: Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with:\n",
    "\n",
    "There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.\n",
    "\n",
    "as the first result. How cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7573b-ae0c-482d-bc04-e9c1cafef46f",
   "metadata": {},
   "source": [
    "### Similarity measures: dot product and cosine similarity\n",
    "   Let's talk similarity measures between vectors. Specifically, embedding vectors which are representations of data with magnitude and direction in high dimensional space (our embedding vectors have 768 dimensions). Two of the most common you'll across are the dot product and cosine similarity. They are quite similar. The main difference is that cosine similarity has a normalization step.\n",
    "   \n",
    "| Similarity measure | Description | Code |\n",
    "| ----- | ----- | ----- |\n",
    "|[Dot Product](https://en.wikipedia.org/wiki/Dot_product) | - Measure of magnitude and direction between two vectors<br>- Vectors that are aligned in direction and magnitude have a higher positive value<br>- Vectors that are opposite in direction and magnitude have a higher negative value | [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html), [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [`sentence_transformers.util.dot_score`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.dot_score) |\n",
    "|[Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | - Vectors get normalized by magnitude/[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics))/L2 norm so they have unit length and are compared more so on direction<br>- Vectors that are aligned in direction have a value close to 1<br>- Vectors that are opposite in direction have a value close to -1 | [`torch.nn.functional.cosine_similarity`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html), [`1 - scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) (subtract the distance from 1 for similarity measure), [`sentence_transformers.util.cos_sim`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim)|\n",
    "   \n",
    "    \n",
    "For text similarity, you generally want to use cosine similarity as you are after the semantic measurements (direction) rather than magnitude. \n",
    "    In our case, our embedding model `all-mpnet-base-v2` outputs normalized outputs (see the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#usage-huggingface-transformers) for more on this) so dot product and cosine similarity return the same results. However, dot product is faster due to not need to perform a normalize step.\n",
    "    To make things bit more concrete, let's make simple dot product and cosine similarity functions and view their results on different vectors.\n",
    "    \n",
    " **Note:** Similarity measures between vectors and embeddings can be used on any kind of embeddings, not just text embeddings. For example, you could measure image embedding similarity or audio embedding similarity. Or with text and image models like [CLIP](https://github.com/mlfoundations/open_clip), you can measure the similarity between text and image embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7de78f-3a5b-4ec8-9b76-72c757033287",
   "metadata": {},
   "source": [
    "### Semantic search/vector search extensions\n",
    "We've covered an exmaple of using embedding vector search to find relevant results based on a query.\n",
    "\n",
    "However, you could also add to this pipeline with traditional keyword search.\n",
    "\n",
    "Many modern search systems use keyword and vector search in tandem.\n",
    "\n",
    "Our dataset is small and allows for an exhaustive search (comparing the query to every possible result) but if you start to work with large scale datasets with hundred of thousands, millions or even billions of vectors, you'll want to implement an index.\n",
    "\n",
    "You can think of an index as sorting your embeddings before you search through them.\n",
    "\n",
    "So it narrows down the search space.\n",
    "\n",
    "For example, it would be inefficient to search every word in the dictionary to find the word \"duck\", instead you'd go straight to the letter D, perhaps even straight to the back half of the letter D, find words close to \"duck\" before finding it.\n",
    "\n",
    "That's how an index can help search through many examples without comprimising too much on speed or quality (for more on this, check out nearest neighbour search).\n",
    "\n",
    "One of the most popular indexing libraries is Faiss.\n",
    "\n",
    "Faiss is open-source and was originally created by Facebook to deal with internet-scale vectors and implements many algorithms such as HNSW (Hierarchical Naviganle Small Worlds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f786f-87fb-45a6-b24b-dd6511d5fb53",
   "metadata": {},
   "source": [
    "### Getting an LLM for local generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b6eaf-b236-45fb-9e42-216cc4d9f725",
   "metadata": {},
   "source": [
    "We're got our retrieval pipeline ready, let's now get the generation side of things happening. To perform generation, we're going to use a Large Language Model (LLM). LLMs are designed to generate an output given an input. In our case, we want our LLM to generate and output of text given a input of text. And more specifically, we want the output of text to be generated based on the context of relevant information to the query.The input to an LLM is often referred to as a prompt.\n",
    "\n",
    "We'll augment our prompt with a query as well as context from our textbook related to that query\n",
    "> **Which LLM should I use?**\n",
    "\n",
    "There are many LLMs available. Two of the main questions to ask from this is:\n",
    "1. Do I want it to run locally? \n",
    "2. If yes, how much compute power can I dedicate?\n",
    "\n",
    "If you're after the absolute best performance, you'll likely want to use an API (not running locally) such as GPT-4 or Claude 3. However, this comes with the tradeoff of sending your data away and then awaiting a response.\n",
    "\n",
    "For our case, since we want to set up a local pipeline and run it on our own GPU, we'd answer \\\"yes\\\" to the first question and then the second question will depend on what hardware we have available.\n",
    "\n",
    "To find open-source LLMs, one great resource is the [Hugging Face open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). The leaderboard compares many of the latest and greatest LLMs on various benchmarks. Another great resource is [TheBloke on Hugging Face](https://huggingface.co/TheBloke), an account which provides an extensive range of quantized (models that have been made smaller) LLMs.\n",
    "\n",
    "A rule of thumb for LLMs (and deep learning models in general) is that the higher the number of parameters, the better the model performs. It may be tempting to go for the largest size model (e.g. a 70B parameter model rather than a 7B parameter model) but a larger size model may not be able to run on your available hardware.\n",
    "\n",
    "The following table gives an insight into how much GPU memory you'll need to load an LLM with different sizes and different levels of [numerical precision](https://en.wikipedia.org/wiki/Precision_(computer_science)).\n",
    "\n",
    "They are based on the fact that 1 float32 value (e.g. `0.69420`) requires 4 bytes of memory and 1GB is approximately 1,000,000,000 (one billion) bytes.\n",
    "\n",
    "| Model Size (Billion Parameters) | Float32 VRAM (GB) | Float16 VRAM (GB) | 8-bit VRAM (GB) | 4-bit VRAM (GB) |\n",
    "|-----|-----|-----|-----|-----|\n",
    "| 1B                              | ~4                | ~2                | ~1              | ~0.5            |\n",
    "| 7B (e.g., [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b), [Gemma 7B](https://huggingface.co/google/gemma-7b-it), [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1))             | ~28               | ~14               | ~7              | ~3.5            |\n",
    "| 10B                             | ~40               | ~20               | ~10             | ~5              |\n",
    "| 70B (e.g, Llama 2 70B)          | ~280              | ~140              | ~70             | ~35             |\n",
    "| 100B                            | ~400              | ~200              | ~100            | ~50             |\n",
    "| 175B                            | ~700              | ~350              | ~175            | ~87.5           |\n",
    "\n",
    "**Note:** Loading a model in a lower precision (e.g. 8-bit instead of float16) generally lowers performance. Lower precision can help to reduce computing requirements, however sometimes the performance degradation in terms of model output can be substantial. Finding the right speed/performance tradeoff will often require many experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9f04e-370d-46d8-a0ae-17eb0ef328d3",
   "metadata": {},
   "source": [
    "### Checking local GPU memory availability\n",
    " \n",
    "Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n",
    "> **Note:** You can also check this with the `!nvidia-smi` command.\n",
    "\n",
    "Looking at the table above, it seems we can run a ~7-10B parameter model in float16 precision pretty comfortably. But we could also run a smaller one if we'd like. Let's try out the recently released (at the time of writing, March 2024) LLM from Google, [Gemma](https://huggingface.co/blog/gemma).\n",
    "\n",
    "Instruction tuning is the process of tuning a raw language model to follow instructions. These are the kind of models you'll find in most chat-based assistants such as ChatGPT, Gemini or Claude. \n",
    "\n",
    "The following table shows different amounts of GPU memory requirements for different verions of the Gemma LLMs with varying levels of precision.\n",
    "\n",
    "| Model             | Precision | Min-Memory (Bytes) | Min-Memory (MB) | Min-Memory (GB) | Recommended Memory (GB) | Hugging Face ID |\n",
    "|-------------------|-----------|----------------|-------------|-------------| ----- | ----- |\n",
    "| [Gemma 2B](https://huggingface.co/google/gemma-2b-it)          | 4-bit     | 2,106,749,952  | 2009.15     | 1.96        | ~5.0 | [`gemma-2b`](https://huggingface.co/google/gemma-2b) or [`gemma-2b-it`](https://huggingface.co/google/gemma-2b-it) for instruction tuned version |\n",
    "| Gemma 2B          | Float16   | 5,079,453,696  | 4844.14     | 4.73        | ~8.0 | Same as above |\n",
    "| [Gemma 7B](https://huggingface.co/google/gemma-7b-it)          | 4-bit     | 5,515,859,968  | 5260.33     | 5.14        | ~8.0 | [`gemma-7b`](https://huggingface.co/google/gemma-7b) or [`gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) for instruction tuned version |\n",
    "| Gemma 7B          | Float16   | 17,142,470,656 | 16348.33    | 15.97       | ~19 | Same as above |\n",
    "\n",
    "> **Note:** `gemma-7b-it` means \\\"instruction tuned\\\", as in, a base LLM (`gemma-7b`) has been fine-tuned to follow instructions, similar to [`Mistral-7B-v0.1`](https://huggingface.co/mistralai/Mistral-7B-v0.1) and [`Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",
    "> There are also further quantized and smaller variants of Gemma (and other LLMs) available in various formats such as GGUF. You can see many of these on [TheBloke account on Hugging Face](https://huggingface.co/TheBloke).\n",
    "> The version of LLM you choose to use will be largely based on project requirements and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746664f-aacc-447b-8dd4-4aefff393c23",
   "metadata": {},
   "source": [
    "Looks like gemma-2b-it (for my local machine with an RTX 4060, change the model_id and use_quantization_config values to suit your needs)!\n",
    "\n",
    "There are plenty of examples of how to load the model on the Hugging Face model card.\n",
    "\n",
    "Good news is, the Hugging Face transformers library has all the tools we need.\n",
    "\n",
    "To load our LLM, we're going to need a few things:\n",
    "\n",
    "1. A quantization config (optional) - This will determine whether or not we load the model in 4bit precision for lower memory usage. The we can create this with the transformers.BitsAndBytesConfig class (requires installing the bitsandbytes library).\n",
    "2. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example gemma-7b-it.\n",
    "3. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the transformers.AutoTokenzier.from_pretrained method and passing it our model ID.\n",
    "4. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the transformers.AutoModelForCausalLM.from_pretrained method and passing it our model ID as well as other various parameters.\n",
    "As a bonus, we'll check if Flash Attention 2 is available using transformers.utils.is_flash_attn_2_available(). Flash Attention 2 speeds up the attention mechanism in Transformer architecture models (which is what many modern LLMs are based on, including Gemma). So if it's available and the model is supported (not all models support Flash Attention 2), we'll use it. If it's not available, you can install it by following the instructions on the GitHub repo.\n",
    "\n",
    "> Note: Flash Attention 2 currently works on NVIDIA GPUs with a compute capability score of 8.0+ (Ampere, Ada Lovelace, Hopper architectures). We can check our GPU compute capability score with torch.cuda.get_device_capability(0).\n",
    "\n",
    "> Note: To get access to the Gemma models, you will have to agree to the terms & conditions on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the Hugging Face CLI/Hugging Face Hub login() function. Once you've done this, you'll be able to download the models. If you're using Google Colab, you can add a Hugging Face token to the \"Secrets\" tab.\n",
    "\n",
    "Downloading an LLM locally can take a fair bit of time depending on your internet connection. Gemma 7B is about a 16GB download and Gemma 2B is about a 6GB download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85675b52-9745-43e8-911a-2581cc8e164e",
   "metadata": {},
   "source": [
    "# Generating text with our LLM\n",
    "We can generate text with our LLM <code>model</code> instance by calling the <code>generate()</code> [method](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig) (this method has plenty of options to pass into it alongside the text) on it and passing it a tokenized input.\n",
    "\n",
    "The tokenized input comes from passing a string of text to our <code>tokenizer</code>.\n",
    "\n",
    "It's important to note that you should use a tokenizer that has been paired with a model.\n",
    "\n",
    "Otherwise if you try to use a different tokenizer and then pass those inputs to a model, you will likely get errors/strange results.\n",
    "\n",
    "For some LLMs, there's a specific template you should pass to them for ideal outputs.\n",
    "\n",
    "For example, the <code>gemma-7b-it</code> model has been trained in a dialogue fashion (instruction tuning).\n",
    "\n",
    "In this case, our <code>tokenizer</code> has a <code>apply_chat_template()</code> [method](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which can prepare our input text in the right format for the model.\n",
    "\n",
    "Let's try it out.\n",
    "\n",
    "Note: The following demo has been modified from the Hugging Face model card for [Gemma 7B](https://huggingface.co/google/gemma-7b-it). Many similar demos of usage are available on the model cards of similar models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789567ef-348f-476d-af81-ca54f57e3416",
   "metadata": {},
   "source": [
    "# Augmenting our prompt with context items\n",
    "What we'd like to do with augmentation is take the results from our search for relevant resources and put them into the prompt that we pass to our LLM.\n",
    "\n",
    "In essence, we start with a base prompt and update it with context text.\n",
    "\n",
    "write a function called prompt_formatter that takes in a query and our list of context items (in our case it'll be select indices from our list of dictionaries inside pages_and_chunks) and then formats the query with text from the context items.\n",
    "\n",
    "We'll apply the dialogue and chat template to our prompt before returning it as well.\n",
    "\n",
    "Note: The process of augmenting or changing a prompt to an LLM is known as prompt engineering. And the best way to do it is an active area of research. For a comprehensive guide on different prompt engineering techniques, resources are the [Prompt Engineering Guide](https://www.promptingguide.ai/), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering) and the paper [Prompt Design and Engineering: Introduction and Advanced Models](https://arxiv.org/abs/2401.14423)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919ef7f-9c32-4a8f-91da-2d66e1c69a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forklift",
   "language": "python",
   "name": "forklift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
